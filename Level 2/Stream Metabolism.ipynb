{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7dce1d6-fd77-41f5-88ed-2a783f4a6c28",
   "metadata": {},
   "source": [
    "* Downloading data from streamPulse\n",
    "* different ways to calculate gas exchange\n",
    "* run the model with old school k estimates\n",
    "* run the model with bayesian estimation\n",
    "* plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ce29d-cd8f-4aa7-8db4-c31cda8d93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dataRetrieval)\n",
    "library(stringr)\n",
    "library(parallel)\n",
    "library(lubridate)\n",
    "library(streamMetabolizer)\n",
    "library(zoo)\n",
    "library(lubridate)\n",
    "library(chron)\n",
    "library(dplyr)\n",
    "library(doBy)\n",
    "library(oce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a90d04-6cfc-4819-bde0-aade442891c1",
   "metadata": {},
   "source": [
    "# Stream Metabolism\n",
    "Ecosystem metabolism is a critical concept and measurement that allows researchers to model the productivity, respiration, and carbon cycling of an ecosystem.\n",
    "\n",
    "We are going to model ecosystem metabolism in streams (or rivers) using the R package _streamMetabolizer_ developed by USGS. Current ecosystem metabolism methods in aquatic ecosystems using dissolved oxygen time series to infer rates of primary production and _aerobic_ respiration.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/streammetabolism.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "This method works by comparing the rate of oxygen increase during the day, when both photosynthesis and respiration are occurring, versus the night, where only respiration is active. We use the equation below to assess how oxygen changes vary with light while taking into account gas exchange between the water and the air across the water surface.\n",
    "\n",
    "$$ \\Delta m_{O_i,d} = \\left( \\frac{GPP_d}{Z_{i,d}} \\times \\frac{PPFD_{i,d}}{PPFD_d} + \\frac{ER_d}{Z_{i,d}} + f_{i,d}(K600_d)(O_{sat,i,d}-m_{O_i,d}) \\right) \\times \\Delta t $$\n",
    "\n",
    "Here is mO<sub>i,d</sub> is modeled DO concentration for timestep i on day _d_, Osat<sub>i,d</sub> is DO saturation for timestep _i_ on day _d_. Î”t is the length of each time step (60 minutes in our example), GPP<sub>d</sub> and ER<sub>d</sub> are average GPP and ER (both in g-O<sub>2</sub> m<sup>-2</sup> d<sup>-1</sup>) on day _d_. _z<sub>i,d</sub> is average cross-sectional depth of each upstream. _K600<sub>d</sub> is estimated standardized gas exchange rate (d<sup>-1</sup>) scaled to the Schmidt number 600 on day _d_. _PFFD<sub>i,d</sub> is photosynthetic photon flux density at timestep _i_ on day _d_. <span style=\"text-decoration:overline\">PPFD<sub>d</sub></span> represents the sum of solar insolation for day _d_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec495dd-8244-4324-bd95-1034a632ca2c",
   "metadata": {},
   "source": [
    "## Getting data prepared\n",
    "There are a lot of data elements to put together so we're going to start by getting a few custom functions defined. The functions below provide some basic conversions (e.g., _radi_, and _bpcalc_). In addition the function _lightest_ models the light insolation at a given point at a given time using only time and location and a model of insolation based on the position of the sun and the passage of light through the atmosphere. This model does not take into account shading at a given site nor will any weather conditions be accounted for. That being said, it is very difficlt to comprehensively measure light _in situ_ and this modeling approach has been demonstrated to add minimal error to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5f107-35e8-4300-9db0-2ea54c8c1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert degrees to radians\n",
    "radi<-function(degrees){(degrees*pi/180)}\n",
    "\n",
    "# function to estimate light\n",
    "lightest<- function (time, lat, longobs, longstd, year ) {\n",
    "  jday<-as.numeric(trunc(time)-as.numeric(as.Date(year)))\n",
    "  E<- 9.87*sin(radi((720*(jday-81))/365)) - 7.53*cos(radi((360*(jday-81))/365)) - 1.5*sin(radi((360*(jday-81))/365))\n",
    "  LST<-as.numeric (time-trunc(time))\n",
    "  ST<-LST+(3.989/1440)*(longstd-longobs)+E/1440\n",
    "  solardel<- 23.439*sin(radi(360*((283+jday)/365)))\n",
    "  hourangle<-(0.5-ST)*360\n",
    "  theta<- acos( sin(radi(solardel)) * sin(radi(lat)) + cos(radi(solardel)) * cos(radi(lat)) * cos(radi(hourangle)) )\n",
    "  suncos<-ifelse(cos(theta)<0, 0, cos(theta))\n",
    "  GI<- suncos*2326\n",
    "  GI\t\n",
    "}\n",
    "\n",
    "\n",
    "####function returns mm of Hg\n",
    "bpcalc<- function(bpst, alt) {\n",
    "  bpst*25.4*exp((-9.80665*0.0289644*alt)/(8.31447*(273.15+15)))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96196f5b-87d6-407a-b73e-afccd5488f1a",
   "metadata": {},
   "source": [
    "Below is a custom function that downloads USGS data similarly to how we accomplished this in a previous module. This function has been written to format USGS data to be quickly added to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c409d-e7fc-4142-8462-1ca72fb7807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_usgs_q_data<-function(usgs_id, site_name){\n",
    "  rm(list=c(\"site_q_data\",\"site_q_data_old\"))\n",
    "  #\tsite_q_data_old<-read.csv(file=paste(site_name, \"_usgs_q_data.csv\",sep=\"\"),header=TRUE)\n",
    "  usgs_id<-as.character(usgs_id)\n",
    "  usgs_q_url<-constructNWISURL(paste(usgs_id),c(\"00060\",\"00065\"),statCd=\"00011\",\"\",\"\",\"iv\")\n",
    "  site_q_data <- importWaterML1(usgs_q_url)\n",
    "  if(nrow(site_q_data)>0){\n",
    "    names(site_q_data)<-c(\"agency_cd\",\"site_no\",\"dateTime\",\"X_00060_00011\",\"X_00060_00011_cd\",\"X_00065_00011\",\"X_00065_00011_cd\",\"tz_cd\")\n",
    "    #\tsite_q_data_comb<-rbind.data.frame(site_q_data_old,site_q_data)\n",
    "    site_q_data_comb<-site_q_data\n",
    "    site_q_data_comb2<-site_q_data_comb[!duplicated(site_q_data_comb$dateTime),]\n",
    "    #\tsite_q_data_old<-read.csv(paste(site_name, \"_usgs_q_data.csv\",sep=\"\"))\n",
    "    #\tnames(site_q_data_old)<-names(site_q_data)\n",
    "    #\tsite_q_data_comb<-rbind(site_q_data, site_q_data_old)\n",
    "    #\tsite_q_data_nodup<-site_q_data_comb[!duplicated(site_q_data_comb$dateTime),]\n",
    "    site_q_data_comb2$dt<-gsub(\"T\", \" \",site_q_data_comb2$dateTime,fixed=TRUE)\n",
    "    site_q_data_comb2$dt<-gsub(\":00.000\", \" \",site_q_data_comb2$dt)\n",
    "    site_q_data_comb2$dt<-gsub(\"-04:00\", \"-0400\",site_q_data_comb2$dt)\n",
    "    site_q_data_comb2$dt<-gsub(\"-05:00\", \"-0500\",site_q_data_comb2$dt)\n",
    "    site_q_data_comb2$dtp <- strptime(site_q_data_comb2$dt, format=\"%Y-%m-%d %H:%M %z\",tz=\"EST\")\n",
    "    site_q_data_comb2$Site<-rep(paste(site_name),nrow(site_q_data_comb2))\n",
    "    return(site_q_data_comb2)\n",
    "    #write.csv(site_q_data_comb2, file=paste(site_name, \"_usgs_q_data.csv\",sep=\"\"),row.names=FALSE)\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520ce47-ff4e-4d77-9e06-8658a8b4a06d",
   "metadata": {},
   "source": [
    "We are going to be modeling ecosystem metabolism in the Farmington River at the USGS gage site in Unionville Connecticut ([01184000](https://waterdata.usgs.gov/monitoring-location/01188090/#parameterCode=00065&period=P7D&showMedian=false)).\n",
    "\n",
    "Now that we've defined this function, we're going to use it to download data from the USGS site at Unionville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941dc8d-63ec-48e8-8683-2819de5534ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "thom_q<-get_usgs_q_data(\"01184000\",\"UNIO\")\n",
    "thom_q$dtp<-as.POSIXct(thom_q$dtp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988da51-25e8-48e1-9caf-e9bb095a1dd7",
   "metadata": {},
   "source": [
    "Elevation is provided so that we can correct local barometric pressure readings to the appropriate altitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c00014-15b7-4c3a-8d8c-49c099d8f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightlat<-41.7555472\n",
    "lightlong<--72.8870417\n",
    "site_id<-\"01184000\"\n",
    "eleva<-66\n",
    "site_n<-\"UNIO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e1df0-cfa5-40a9-8b53-cc9487a7f124",
   "metadata": {},
   "source": [
    "Now we're going to import the barometric pressure dataset provided, convert and correct for elevation and then merge those data into the discharge dataset we've already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75008f93-70b2-4af0-a3f0-cc30579776b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdl_pressure<-readRDS(\"files/bdl_pressure_2014_2020_15.rds\")\n",
    "names(bdl_pressure)<-c(\"dtp\",\"mslp_mmhg\")\n",
    "bdl_pressure$atmo_inhg<-bdl_pressure$mslp_mmhg/25.4\n",
    "bdl_pressure$site_mmhg<-bpcalc(bdl_pressure$atmo_inhg,eleva)\n",
    "\n",
    "thom_pressure<-merge(thom_q,bdl_pressure,all.x=TRUE,by=\"dtp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53a652-36a4-464f-9b39-68bcbdea3c54",
   "metadata": {},
   "source": [
    "Now we're going to load a dataset containing time series data from a sonde deployed in the Farmington River. Data include temperature, dissolved oxygen, conductance, and pH, though our analysis will focus on the first two variables.\n",
    "\n",
    "We are going to take the sonde data and merge it into the other datasets.\n",
    "\n",
    "Next comes a little housekeeping to make sure our variables are correct named and in the correct units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb30e7-a05f-4243-95c7-8dcc8b2170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_sonde<-read.csv(\"files/unio.csv\",header=TRUE)\n",
    "site_sonde$dtp<-as.POSIXct(site_sonde$est,format=\"%m/%d/%y %H:%M\",tz=\"EST\")\n",
    "#attr(site_sonde$dtp,\"tzone\") <- \"EST\"\n",
    "\n",
    "sonde_q_pressure<-merge(site_sonde,thom_pressure,by=\"dtp\",all.x=TRUE)\n",
    "\n",
    "sonde_q_pressure$q_cms<-sonde_q_pressure$X_00060_00011*028316847\n",
    "sonde_q_pressure$SpCond_mS_cm<-sonde_q_pressure$SpCond_uS_cm/1000\n",
    "\n",
    "#converting conductance to salinity for estimating DO saturation.\n",
    "sonde_q_pressure$sal<-swSCTp(sonde_q_pressure$SpCond_mS_cm,sonde_q_pressure$Temp_deg_C,pressure=sonde_q_pressure$site_mmhg,conductivityUnit=\"mS/cm\")\n",
    "\n",
    "\n",
    "sonde_q_pressure$bp_atm_mb<-sonde_q_pressure$site_mmhg*1.33322\n",
    "sonde_q_pressure$o2sat<-calc_DO_sat(sonde_q_pressure$Temp_deg_C,sonde_q_pressure$bp_atm_mb,sonde_q_pressure$sal,model = \"garcia-benson\")\n",
    "\n",
    "sonde_q_pressure$do_per<-((sonde_q_pressure$HDO_mg_l/sonde_q_pressure$o2sat)*100)\n",
    "#sonde_q_pressure$dtp_lag<-dplyr::lag(sonde_q_pressure$dtp,k=1)\n",
    "#sonde_q_pressure$timestep<-sonde_q_pressure$dtp-sonde_q_pressure$dtp_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e2c56-60c8-47e1-935c-0d0f93b80ff6",
   "metadata": {},
   "source": [
    "Now we need to obtain channel geometry information from USGS (e.g., channel width and channel area) so that we can estimate mean river depth so that it can be incorporated into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fff644-6a65-448b-949b-c2ba77f8c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_url<-paste(\"https://waterdata.usgs.gov/nwis/measurements?site_no=\",site_id,\"&agency_cd=USGS&format=rdb_expanded\",sep=\"\")\n",
    "chan_data <- importRDB1(chan_url)\n",
    "\n",
    "chan_data_1<-subset(chan_data,chan_velocity!=\"\")\n",
    "chan_data_1$chan_area<-as.numeric(as.character(chan_data_1$chan_area))\n",
    "chan_data_1$chan_width<-as.numeric(as.character(chan_data_1$chan_width))\n",
    "chan_data_1$chan_width_old<-chan_data_1$chan_width\n",
    "#chan_data_1$mean_depth<-(chan_data_1$chan_area/chan_data_1$chan_width)-0.2\n",
    "chan_data_1$mean_depth<-(chan_data_1$chan_area/chan_data_1$chan_width)\n",
    "#chan_data_1$chan_width<-chan_data_1$chan_area/chan_data_1$mean_depth\n",
    "gage_mdepth<-lm(log(chan_data_1$mean_depth)~log(chan_data_1$chan_discharge),na.action=na.omit)\n",
    "gage_vel<-lm(chan_data_1$chan_velocity~chan_data_1$gage_height_va,na.action=na.omit)\n",
    "gage_width<-lm(chan_data_1$chan_width~chan_data_1$gage_height_va,na.action=na.omit)\n",
    "\n",
    "\n",
    "sonde_q_pressure$mdepth_f<-exp((log(sonde_q_pressure$X_00060_00011)*gage_mdepth$coefficients[2])+gage_mdepth$coefficients[1])\n",
    "sonde_q_pressure$width_f<-(sonde_q_pressure$X_00065_00011*gage_width$coefficients[2])+gage_width$coefficients[1]\n",
    "sonde_q_pressure$vel_fts<-(sonde_q_pressure$X_00065_00011*gage_vel$coefficients[2])+gage_vel$coefficients[1]\n",
    "\n",
    "sonde_q_pressure$chr0n<-chron(dates = strftime(sonde_q_pressure$dt, format=\"%Y-%m-%d\",tz=\"EST\"), times = strftime(sonde_q_pressure$dt, format=\"%H:%M:%S\",tz=\"EST\"), format=c(dates = \"y-m-d\", times = \"h:m:s\"))\n",
    "sonde_q_pressure$TIME<-strftime(sonde_q_pressure$dt, format=\"%H:%M:%S\",tz=\"EST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0686c2d-7761-48de-a313-b7434e219df7",
   "metadata": {},
   "source": [
    "Essentially all real-world datasets have gaps of some sort. To run this model, we need to remove these gaps. The block of code below subsets each day of data and interpolates gaps that are 2 hours or less in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9b271-3d36-41db-8a9b-9719455bdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_phel_sonde_1<-subset(sonde_q_pressure,!is.na(X_00065_00011) & !is.na(sonde_q_pressure$vel_fts) & q_cms>0)\n",
    "p_phel_sonde_o<-p_phel_sonde_1[order(p_phel_sonde_1$dt),]\n",
    "date_index<-unique(dates(p_phel_sonde_o$chr0n))\n",
    "date_len<-length(date_index)-1\n",
    "\n",
    "dates_list<-p_phel_sonde_o\n",
    "dates_list$date<-as.Date(dates_list$dtp)\n",
    "date_run<-summaryBy(date~date,dates_list,FUN=c(length))\n",
    "\n",
    "\n",
    "\n",
    "rm(accum)\n",
    "for(i in 2:date_len){\n",
    "  workd00<-subset(p_phel_sonde_o, p_phel_sonde_o$chr0n>=as.numeric(chron(dates=format(as.Date(date_index)[i-1],\"%m/%d/%y\"), times=\"22:00:00\")) & p_phel_sonde_o$chr0n<=as.numeric(chron(dates=format(as.Date(date_index)[i+1],\"%m/%d/%y\"), times=\"06:00:00\")))\n",
    "  workd0<-workd00[!is.na(workd00$Temp_deg_C),]\n",
    "  #\tworkd0<-workd0[!is.na(workd0$mdepth_f),]\n",
    "  #\tworkd0<-workd0[!is.na(workd0$p_abp_mmhg),]\n",
    "  #\tworkd0<-workd0[!is.na(workd0$light),]\n",
    "  t0<-chron(dates=format(as.Date(date_index)[i-1],\"%m/%d/%y\"), times=\"22:00:00\")\n",
    "  t1<-chron(dates=format(as.Date(date_index)[i+1],\"%m/%d/%y\"), times=\"06:00:00\")\n",
    "  cindex<-seq(t0,t1+(1/24/60),by=(1/24/4))\n",
    "  ci<-as.data.frame(cindex)\n",
    "  workd0$datematch<-as.POSIXct(workd0$chr0n,tz=\"EST\")\n",
    "  ci$datematch<-as.POSIXct(ci$cindex,tz=\"EST\")\n",
    "  ci<-zoo(ci)\n",
    "  if(sum(!is.na(workd0$HDO_mg_l))>23){\n",
    "    #\tworkd<-merge(ci,workd0,by.x=\"cindex\",by.y=\"chr0n\",all.x=TRUE, all.y=FALSE)\n",
    "    workd<-merge(ci,workd0,by=\"datematch\",all.x=TRUE)\n",
    "    #\tworkd$TIME<-strftime(workd$cindex+(5/24), tz=\"EST\", format=\"%H:%M:%S\")\n",
    "    #\tworkd$dtp3<-as.POSIXct(workd$cindex)\n",
    "    workd$Temp_deg_C<-na.approx(workd$Temp_deg_C,rule=2)\n",
    "    workd$mdepth_f<-na.approx(workd$mdepth_f,rule=2)\n",
    "    workd$vel_fts<-na.approx(workd$vel_fts,rule=2)\n",
    "    #workd$width_f<-na.approx(workd$width_f,rule=2)\n",
    "    workd$site_mmhg<-na.approx(workd$site_mmhg,rule=2)\n",
    "    #\tworkd$light<-na.approx(workd$light,rule=2)\n",
    "    workd$HDO_mg_l<-na.approx(workd$HDO_mg_l,rule=2)\n",
    "    workd$o2sat<-na.approx(workd$o2sat,rule=2)\n",
    "    workd$stage_ft_fill<-na.approx(workd$X_00065_00011,rule=2)\t\n",
    "    workd$q_cfs_fill<-na.approx(workd$X_00060_00011,rule=2)\t\n",
    "    workd1<-as.data.frame(workd)\n",
    "    workd1$dtp3<-as.POSIXct(workd1$datematch.ci,tz=\"EST\")\t\n",
    "    workd1$chr0n<-chron(dates = strftime(workd1$dtp3, format=\"%Y-%m-%d\",tz=\"EST\"), times = strftime(workd1$dtp3, format=\"%H:%M:%S\",tz=\"EST\"), format=c(dates = \"y-m-d\", times = \"h:m:s\"))\n",
    "    workd1$light<- lightest(time=workd1$chr0n, lat=lightlat,  longobs=-lightlong, longstd= 75, year=\"2016-01-01\")\t\n",
    "  }\n",
    "  if(!exists(\"accum\")){accum<-workd1[0,]}\n",
    "  accum<-bind_rows(accum,workd1)\n",
    "  print(i)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6b17c-5088-4bbd-9ae5-adeeb574028a",
   "metadata": {},
   "source": [
    "This step does some additional formatting, ensuring that all the variables have the right name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3043996-bc78-44e6-84e9-7b8c45e671bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum$discharge<-as.numeric(as.character(accum$q_cfs_fill))*0.0283168\n",
    "accum3<-subset(accum,!is.na(dtp3) & !is.na(discharge))\n",
    "#accum3<-accum2[!is.na(accum2$dtp3),]\n",
    "attr(accum3$dtp3,\"tzone\") <- \"Etc/GMT+5\"\n",
    "accum3$solar.time <- streamMetabolizer::calc_solar_time(accum3$dtp3, longitude=lightlong)\n",
    "#accum3$solar.time<-local2Solar(accum3$dtp3,lon=lightlong)\n",
    "\n",
    "accum3$mdepth_m<-(as.numeric(as.character(accum3$mdepth_f))*0.3048)+1\n",
    "accum4<-accum3[order(accum3$solar.time),]\n",
    "accum5<-accum4[!duplicated(accum4$solar.time),]\n",
    "dat2<-subset(accum5,select=c(\"solar.time\",\"HDO_mg_l\",\"o2sat\",\"mdepth_m\",\"Temp_deg_C\",\"light\"))\n",
    "names(dat2)<-c(\"solar.time\",\"DO.obs\",\"DO.sat\",\"depth\",\"temp.water\",\"light\")\n",
    "\n",
    "dat2q<-subset(accum5,select=c(\"solar.time\",\"HDO_mg_l\",\"o2sat\",\"mdepth_m\",\"Temp_deg_C\",\"light\",\"discharge\"))\n",
    "names(dat2q)<-c(\"solar.time\",\"DO.obs\",\"DO.sat\",\"depth\",\"temp.water\",\"light\",\"discharge\")\n",
    "\n",
    "\n",
    "dat2q$DO.obs<-as.numeric(as.character(dat2q$DO.obs))\n",
    "dat2q$DO.sat<-as.numeric(as.character(dat2q$DO.sat))\n",
    "dat2q$temp.water<-as.numeric(as.character(dat2q$temp.water))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab593d-481f-4723-b28f-5024fefb2071",
   "metadata": {},
   "source": [
    "The most challenging part of ecosystem metabolism in streams and rivers isn't measuring the biology of the system, it's accounting for the exchange of oxygen between water and air. This must be accounted for to generate an accurate model.\n",
    "\n",
    "We are going to introduce two approaches for doing this. The first is to manually estimate gas exchange rate (K) using empirical equations that are based on the flow rate of water.\n",
    "\n",
    "The second, more sophisticated approach, uses Bayesian estimation to pool estimates of K made by the ecosystem metabolism model across days, developing a relationship between discharge and K that is derived from this dataset.\n",
    "\n",
    "For more inormation on this approach, check out the [streamMetabolizer documentation](https://web.archive.org/web/20210325145225/https://usgs-r.github.io/streamMetabolizer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e32346-0159-45d8-bd64-cd028f51993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes_name <- mm_name(type='bayes', pool_K600='binned',err_obs_iid=TRUE, err_proc_acor=FALSE, err_proc_iid=TRUE, ode_method='trapezoid')\n",
    "bayes_name<-\"b_Kb_oipi_tr_plrckm.stan\"\n",
    "bayes_name\n",
    "\n",
    "#bayes_specs <- specs(bayes_name)\n",
    "#bayes_specs\n",
    "\n",
    "#we are taking the natural log of the discharge record that will then be placed into evenly spaced bins for Bayesian estimation.\n",
    "ln.disch <- log(thom_q$discharge)\n",
    "\n",
    "# for use in setting specs\n",
    "brks <- calc_bins(ln.disch, 'width', width=0.2)$bounds\n",
    "specs('b_Kb_oipi_tr_plrckm.stan', K600_lnQ_nodes_centers=brks)\n",
    "\n",
    "\n",
    "\n",
    "#brks<-seq(3.66,7.88,0.2)\n",
    "# one way to alter specifications: call specs() again\n",
    "bayes_specs <- specs(bayes_name, K600_lnQ_nodes_centers=brks, burnin_steps=1000,saved_steps=2000,verbose=TRUE)\n",
    "#bayes_specs <- specs(bayes_name, K600_lnQ_nodes_centers=brks, burnin_steps=10,saved_steps=10)\n",
    "\n",
    "phel_q$DO.obs<-as.numeric(phel_q$DO.obs)\n",
    "phel_q$DO.sat<-as.numeric(phel_q$DO.sat)\n",
    "phel_q$temp.water<-as.numeric(phel_q$temp.water)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Finally we are going to use streamMetabolizer to fit the Bayesian model.\n",
    "bayes_fit <- streamMetabolizer::metab(specs = bayes_specs, data=phel_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3925d-402d-48d9-b721-c211a9aa67bd",
   "metadata": {},
   "source": [
    "Now we're going to plot the comparison between these two different methods of estimating GPP and ER in streams an rivers so we can understand how these different methods impact model estimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
