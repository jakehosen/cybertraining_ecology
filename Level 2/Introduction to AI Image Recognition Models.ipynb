{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4354e0ed-a654-492b-a4ff-4f70840a38ff",
   "metadata": {},
   "source": [
    "## AI Image Recognition Model Introduction\n",
    "\n",
    "We are using the modeling framework of [Walhtinez and Walhtinez (2024)]( https://doi.org/10.1111/2041-210X.14278)\n",
    "\n",
    "### First Step: Loading libraries for Python and Getting our workspace ready.\n",
    "Note that we are identifying common suffixes for image files. We have to identify each version separately. Thus for jpeg files we have to indicate both \"jpg\" and \"jpeg\", because both variations are commonly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c4979-27a1-450b-a0ed-ce4ee3548fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import functools\n",
    "import math\n",
    "import pathlib\n",
    "import tempfile\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "from absl import logging\n",
    "from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SUFFIXES = (\"jpg\", \"jpeg\", \"gif\", \"png\")\n",
    "\n",
    "\n",
    "def _iter_folder_images(path: pathlib.Path) -> Iterator[str]:\n",
    "  yield from (p for p in path.iterdir() if p.is_file() and p.suffix[1:] in IMAGE_SUFFIXES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4dfa0-6d1c-4c00-b8c8-b21fad2f51db",
   "metadata": {},
   "source": [
    "### Defining functions we will use.\n",
    "As you can see above, the TensorFlow library will be used. This is a popular open source library for machine learning and AI tasks, but it only  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd98649-2972-4723-ab54-7ef3027f58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_folder_images(path: pathlib.Path) -> Iterator[str]:\n",
    "  yield from (p for p in path.iterdir() if p.is_file() and p.suffix[1:] in IMAGE_SUFFIXES)\n",
    "\n",
    "\n",
    "def triplet_safe_image_dataset_from_directory(\n",
    "    path: str,\n",
    "    image_size: Tuple[int, int] = None,\n",
    "    shuffle: bool = True,\n",
    "    batch_size: int = 32,\n",
    "    color_mode: str = \"rgb\",\n",
    "    seed: float = None,\n",
    ") -> tf.data.Dataset:\n",
    "  assert color_mode in (\"rgb\", \"rgba\", \"grayscale\")\n",
    "\n",
    "  # Instantiate the random number generator with known seed.\n",
    "  rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "  # Inspect subdirectories and list all available images.\n",
    "  subdirs = [d for d in pathlib.Path(path).iterdir() if d.is_dir()]\n",
    "  class_names = [d.name for d in subdirs]\n",
    "  class_labels = list(range(len(class_names)))\n",
    "  paths_by_label = {i: list(_iter_folder_images(d)) for i, d in zip(class_labels, subdirs)}\n",
    "\n",
    "  file_paths = list(sorted(sum(paths_by_label.values(), [])))\n",
    "  logging.info(f\"Found {len(file_paths)} files belonging to {len(class_names)} classes.\")\n",
    "\n",
    "  # Define inner functions that make use of provided arguments implicitly.\n",
    "  def _read_image(file_path: str) -> tf.Tensor:\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_image(img, channels=3)\n",
    "    if color_mode == \"grayscale\":\n",
    "      img = tf.image.rgb_to_grayscale(img)\n",
    "    if image_size is not None:\n",
    "      img = tf.image.resize(img, image_size)\n",
    "      img = tf.reshape(img, tuple([*image_size, -1]))\n",
    "    return img\n",
    "\n",
    "  def _iter_samples() -> Iterator[Tuple[pathlib.Path, int]]:\n",
    "    for i, paths in paths_by_label.items():\n",
    "      for p in paths:\n",
    "        yield (p, i)\n",
    "\n",
    "  def _sample_generator() -> Iterator[Tuple[tf.Tensor, int]]:\n",
    "    samples = list(_iter_samples())\n",
    "    for file_path_1, label in rng.permutation(samples) if shuffle else samples:\n",
    "      possible_file_paths = [f for f in paths_by_label[label] if f != file_path_1]\n",
    "      if possible_file_paths:\n",
    "        file_path_2 = rng.choice(possible_file_paths)\n",
    "        yield (_read_image(str(file_path_1)), label)\n",
    "        yield (_read_image(str(file_path_2)), label)\n",
    "\n",
    "  spec = (\n",
    "      tf.TensorSpec(shape=(None, None, 3), dtype=tf.uint8),\n",
    "      tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "  )\n",
    "  ds = tf.data.Dataset.from_generator(_sample_generator, output_signature=spec).batch(batch_size)\n",
    "\n",
    "  # Set certain attributes similarly to tf.keras.utils.image_dataset_from_directory.\n",
    "  setattr(ds, \"file_paths\", file_paths)\n",
    "  setattr(ds, \"class_names\", class_names)\n",
    "  return ds\n",
    "\n",
    "\n",
    "def load_dataset(\n",
    "    path: pathlib.Path,\n",
    "    splits: List[float],\n",
    "    batch_size: int = 32,\n",
    "    seed: float = None,\n",
    "    **kwargs,\n",
    ") -> List[tf.data.Dataset]:\n",
    "  assert abs(sum(splits) - 1) < 1e3, \"Split fractions must add up to one\"\n",
    "  splits_cumsum = [sum(splits[:i]) for i in range(1, len(splits))]\n",
    "\n",
    "  # Produce a list of all samples grouped by label.\n",
    "  subdirs = [d.absolute() for d in pathlib.Path(path).iterdir() if d.is_dir()]\n",
    "\n",
    "  rng = np.random.RandomState(seed=seed)\n",
    "  split_indices = [math.ceil(frac * len(subdirs)) for frac in splits_cumsum]\n",
    "  group_splits = list(np.split(rng.permutation(subdirs), split_indices))\n",
    "\n",
    "  # Write the resulting splits into a temporary directory.\n",
    "  tmpdir = pathlib.Path(tempfile.mkdtemp())\n",
    "  for idx, group in enumerate(group_splits):\n",
    "    (tmpdir / str(idx)).mkdir()\n",
    "    for d in group:\n",
    "      (tmpdir / str(idx) / d.name).symlink_to(d, target_is_directory=True)\n",
    "\n",
    "  # Read the temporary directories into a tf dataset.\n",
    "  ds_opts = dict(shuffle=True, color_mode=\"rgb\", batch_size=batch_size, **kwargs)\n",
    "\n",
    "  # Training and validation sets require distribution of samples that allow for triple mining.\n",
    "  output = []\n",
    "  for i in range(len(splits) - 1):\n",
    "    img_dir = tmpdir / str(i)\n",
    "    output.append(triplet_safe_image_dataset_from_directory(img_dir, **ds_opts))\n",
    "\n",
    "  # Test set can use the normal sampling procedure.\n",
    "  ds_test_func = tf.keras.utils.image_dataset_from_directory\n",
    "  output.append(ds_test_func(tmpdir / str(len(splits) - 1), follow_links=True, **ds_opts))\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def download_from_gcloud(\n",
    "    bucket_name: str,\n",
    "    output_dir: str,\n",
    "    prefix: str = \"\",\n",
    "    parallelism: int = 32,\n",
    ") -> None:\n",
    "  futures = []\n",
    "  client = storage.Client()\n",
    "  bucket = client.bucket(bucket_name)\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers=parallelism) as executor:\n",
    "    for blob in bucket.list_blobs(prefix=prefix):\n",
    "      output_path = pathlib.Path(output_dir) / blob.name\n",
    "      output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "      future = executor.submit(blob.download_to_filename, output_path)\n",
    "      futures.append(future)\n",
    "\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    if future.exception():\n",
    "      raise future.exception()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
